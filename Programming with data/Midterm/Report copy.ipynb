{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mid term Report title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import yfinance as yf\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SentimentAnalyzer\n",
    "sentimentAnalyzer = SentimentAnalyzer()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline  \n",
    "# using the style for the plot\n",
    "plt.style.use('dark_background')\n",
    "# set plot size\n",
    "plt.rcParams['figure.figsize'] = [30, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scope Limits\n",
    "StartTime = \"2012-05-01\"\n",
    "EndTime = \"2022-05-01\"\n",
    "TweetLimit = 1000000 #this is roughly 200 MB\n",
    "\n",
    "# scope of a jump\n",
    "MinDaysBetweenJumps = 7\n",
    "\n",
    "StockJumpTriggerRatio = 0.5\n",
    "StockJumpTriggerPercentage = 1\n",
    "\n",
    "TwitterJumpTriggerRatio = 1.25\n",
    "TwitterJumpTriggerPercentage = 7.5\n",
    "\n",
    "#scope of number of tweets\n",
    "UsingHashTags = True\n",
    "UsingPhrases = False\n",
    "\n",
    "\n",
    "# caching folders\n",
    "TweetCacheFolder_Small = \"CachedTweets_Small\"\n",
    "TweetCacheFolder_Large = \"CachedTweets_Large\"\n",
    "StockCacheFolder = \"CachedStockData\"\n",
    "\n",
    "TweetCacheDict = {}\n",
    "StockCacheDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what users do we care about\n",
    "\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "\tcompaniesToReview = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StrToDate(date):\n",
    "\ttry:\n",
    "\t\tdate = parser.parse(date)\n",
    "\texcept:\n",
    "\t\traise Exception(\"invalid date: \" + str(date) + \" Of type: \" + str(type(date)))\n",
    "\t\n",
    "\t\n",
    "\tdate = datetime(date.year, date.month, date.day)\n",
    "\treturn date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTweetSentiment(text):\n",
    "\tscores = sentimentAnalyzer.polarity_scores(text)\n",
    "\treturn scores[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTweetsFromAboutCompany(companyData, isPublicOpinion=False):\n",
    "\n",
    "\ttweetCacheFolder = TweetCacheFolder_Small\n",
    "\tdictKey = companyData[\"StockName\"]\n",
    "\n",
    "\tif isPublicOpinion: \n",
    "\t\ttweetCacheFolder =TweetCacheFolder_Large\n",
    "\t\tdictKey += \"_isPublicOpinion\"\n",
    "\n",
    "\tif not os.path.isdir(tweetCacheFolder):\n",
    "\t\tos.mkdir(tweetCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(tweetCacheFolder, \"tweets_\" + companyData[\"StockName\"] + \".csv\")\n",
    "\n",
    "\t#dict cache exists\n",
    "\tif dictKey in TweetCacheDict:\n",
    "\t\tdf = TweetCacheDict[dictKey]\n",
    "\n",
    "\t# file cache exists\n",
    "\telse:\n",
    "\t\tif os.path.exists(cachePath):\n",
    "\t\t\tdf = pd.read_csv(cachePath)\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# build the query\n",
    "\t\t\tquery = \"(\"\n",
    "\n",
    "\t\t\ttwitterUsernames = companyData[\"TwitterUsernames\"]\n",
    "\t\t\ttwitterHashTags = companyData[\"TwitterHashTags\"]\n",
    "\t\t\ttwitterPhrases = companyData[\"TwitterPhrases\"]\n",
    "\n",
    "\t\t\tfor i in range(len(twitterUsernames)):\n",
    "\t\t\t\tquery += \"from:\" + twitterUsernames[i]\n",
    "\t\t\t\tif i < len(twitterUsernames) - 1:\n",
    "\t\t\t\t\tquery += \" OR \"\n",
    "\n",
    "\t\t\tif UsingHashTags:\n",
    "\t\t\t\tquery += \") OR (\"\n",
    "\n",
    "\t\t\t\tfor i in range(len(twitterHashTags)):\n",
    "\t\t\t\t\tquery += twitterHashTags[i]\n",
    "\t\t\t\t\tif i < len(twitterHashTags) - 1:\n",
    "\t\t\t\t\t\tquery += \" OR \"\n",
    "\n",
    "\t\t\tif UsingPhrases:\n",
    "\t\t\t\tquery += \") OR (\"\n",
    "\n",
    "\t\t\t\tfor i in range(len(twitterPhrases)):\n",
    "\t\t\t\t\tquery += twitterPhrases[i]\n",
    "\t\t\t\t\tif i < len(twitterPhrases) - 1:\n",
    "\t\t\t\t\t\tquery += \" OR \"\n",
    "\n",
    "\t\t\tquery += \") since:\" + StartTime + \" until:\" + EndTime + \" -filter:links -filter:replies\"\n",
    "\t\t\t\n",
    "\t\t\t# get tweets that match the query\n",
    "\t\t\ttweets = []\n",
    "\t\t\tfor tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\t\t\t\t\n",
    "\t\t\t\tif TweetLimit > 0 and len(tweets) == TweetLimit:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsentiment = GetTweetSentiment(tweet.content)\n",
    "\t\t\t\t\ttweets.append([tweet.date, tweet.user.username, tweet.content, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.id, sentiment])\n",
    "\n",
    "\t\t\tdf = pd.DataFrame(tweets, columns=['Date', 'User', 'Content', 'ReplyCount', 'RetweetCount', 'LikeCount', 'QuoteCount', 'TweetID', 'Sentiment'])\n",
    "\n",
    "\t\t\t# to save to csv as a cache\n",
    "\t\t\tdf.to_csv(cachePath)\n",
    "\n",
    "\t\tfor i in range(len(df)):\n",
    "\t\t\tdf[\"Date\"][i] = StrToDate(df[\"Date\"][i])\n",
    "\n",
    "\t\tdf = df.sort_values(by=['Date'])\n",
    "\t\tTweetCacheDict[dictKey] = df\n",
    "\t\n",
    "\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadStockData(stockTag):\n",
    "\n",
    "\tif not os.path.isdir(StockCacheFolder):\n",
    "\t\tos.mkdir(StockCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(StockCacheFolder, \"Stock_\" + stockTag + \".csv\")\n",
    "\n",
    "\t#dict cache exists\n",
    "\tif stockTag in TweetCacheDict:\n",
    "\t\tstockDataDf = TweetCacheDict[stockTag]\n",
    "\n",
    "\t# file cache exists\n",
    "\telse:\n",
    "\t\tif os.path.exists(cachePath):\n",
    "\t\t\tstockDataDf = pd.read_csv(cachePath)\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tstockDataDf = yf.download(stockTag, start=StartTime, end=EndTime, progress=False)\n",
    "\n",
    "\t\t\t#save data to local cache file\n",
    "\t\t\tstockDataDf.to_csv(cachePath)\n",
    "\n",
    "\n",
    "\t\tfor i in range(len(stockDataDf)):\n",
    "\t\t\tstockDataDf[\"Date\"][i] = StrToDate(stockDataDf[\"Date\"][i])\n",
    "\n",
    "\t\tstockDataDf = stockDataDf.sort_values(by=['Date'])\n",
    "\t\tTweetCacheDict[stockTag] = stockDataDf\n",
    "\n",
    "\t\n",
    "\treturn stockDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMovingAverage(df, columnToAverage, averageLength):\n",
    "\n",
    "\tmovingAverage = []\n",
    "\tmovingAverageUpperQuantile = []\n",
    "\tmovingAverageLowerQuantile = []\n",
    "\n",
    "\trows = df.loc[:,columnToAverage]\n",
    "\tfor i in range(len(rows)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < averageLength:\n",
    "\t\t\twindow = [0]\n",
    "\t\telse:\n",
    "\t\t\twindow = rows[i - averageLength : i]\n",
    "\n",
    "\t\t# calculate moving avg\n",
    "\t\twindowAverage = round(sum(window) / averageLength, 2)\n",
    "\t\tmovingAverage.append(windowAverage)\n",
    "\n",
    "\t\t# calculate UpperQuantile\n",
    "\t\tupperQuantile = np.quantile(window, 0.9)\n",
    "\t\tmovingAverageUpperQuantile.append(upperQuantile)\n",
    "\n",
    "\t\t# calculate LowerQuantile\n",
    "\t\tlowerQuantile = np.quantile(window, 0.1)\n",
    "\t\tmovingAverageLowerQuantile.append(lowerQuantile)\n",
    "\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength), movingAverage, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_UpperQuantile\", movingAverageUpperQuantile, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_LowerQuantile\", movingAverageLowerQuantile, allow_duplicates=True)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectValueJumping(df, column, lastValueColumn, currentValueColumn, jumpTriggerRatio, jumpTriggerPercentage):\n",
    "\tisValueJumping = []\n",
    "\n",
    "\tfor i in range(len(df)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < 45: #skip the first few days as the moving averages take a few days to settle\n",
    "\t\t\tisJump = False\n",
    "\t\telse:\n",
    "\t\t\tupperQuantile = df[column + \"MA_7MA_30_UpperQuantile\"][i]\n",
    "\t\t\tlowerQuantile = df[column + \"MA_7MA_30_LowerQuantile\"][i]\n",
    "\t\t\tlastValue = df[lastValueColumn][i]\n",
    "\t\t\tcurrentValue = df[currentValueColumn][i]\n",
    "\n",
    "\t\t\tinterQuantileRange = upperQuantile - lowerQuantile\n",
    "\t\t\ttriggerRange = jumpTriggerRatio * interQuantileRange\n",
    "\n",
    "\t\t\tmeantRatio = currentValue > upperQuantile + triggerRange or currentValue < lowerQuantile - triggerRange\n",
    "\n",
    "\t\t\tmeantPercentage = False\n",
    "\t\t\tif lastValue != 0:\n",
    "\t\t\t\tmeantPercentage = abs(lastValue - currentValue) / lastValue > jumpTriggerPercentage / 100\n",
    "\t\t\t\n",
    "\t\t\tisJump = meantRatio and meantPercentage\n",
    "\n",
    "\t\tisValueJumping.append(isJump)\n",
    "\n",
    "\tdf.insert(len(df.columns), column + \"_IsJumping\", isValueJumping, allow_duplicates=True)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def CreateStockTrendData(stockData):\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 7)\n",
    "\tCreateMovingAverage(stockData, \"VolumeMA_7\", 30)\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Open\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Open\", 7)\n",
    "\tCreateMovingAverage(stockData, \"OpenMA_7\", 30)\n",
    "\n",
    "\tDetectStockJumps(stockData)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def DetectStockJumps(stockData):\n",
    "\n",
    "\tDetectValueJumping(stockData, \"Volume\", \"VolumeMA_7\", \"VolumeMA_3\",\n",
    "\t\tStockJumpTriggerRatio, StockJumpTriggerPercentage)\n",
    "\tDetectValueJumping(stockData, \"Open\", \"OpenMA_3\", \"Open\",\n",
    "\t\tStockJumpTriggerRatio, StockJumpTriggerPercentage)\n",
    "\n",
    "\t\n",
    "\tbothJumped = stockData[\"Volume_IsJumping\"] * stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Both_IsJumping\", bothJumped, allow_duplicates=True)\n",
    "\n",
    "\teitherJumped = stockData[\"Volume_IsJumping\"] + stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Either_IsJumping\", eitherJumped, allow_duplicates=True)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def DetectTweetJumps(tweetData):\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 3)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 7)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 30)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentimentMA_7\", 30)\n",
    "\n",
    "\tDetectValueJumping(tweetData, \"avgSentiment\", \"avgSentimentMA_7MA_30\", \"avgSentimentMA_7\",\n",
    "\t\tTwitterJumpTriggerRatio, TwitterJumpTriggerPercentage)\n",
    "\n",
    "\treturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetValueJumpsDates(stockData, jumpColumn):\n",
    "\n",
    "\tdates = []\n",
    "\tdaysSinceLastJump = MinDaysBetweenJumps\n",
    "\n",
    "\tfor i in range(len(stockData)):\n",
    "\t\tdaysSinceLastJump += 1\n",
    "\n",
    "\t\tisJump = stockData[jumpColumn][i]\n",
    "\n",
    "\t\tif isJump and daysSinceLastJump >= MinDaysBetweenJumps:\n",
    "\t\t\tdates.append(stockData[\"Date\"][i])\n",
    "\t\t\tdaysSinceLastJump = 0\n",
    "\n",
    "\treturn dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#down load all needed data to local cache\n",
    "for companyData in companiesToReview:\n",
    "\tGetTweetsFromAboutCompany(companyData)\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companyData, isPublicOpinion=True)\n",
    "\n",
    "\tDetectTweetJumps(tweetData)\n",
    "\n",
    "\tcompanyData[\"TweetJumpDates\"] = GetValueJumpsDates(tweetData, \"avgSentiment_IsJumping\")\n",
    "\t\n",
    "\n",
    "\tstockData = LoadStockData(companyData[\"Tag\"])\n",
    "\tCreateStockTrendData(stockData)\n",
    "\tcompanyData[\"StockJumpDates\"] = GetValueJumpsDates(stockData, \"Both_IsJumping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Open Price / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"Open\"], label = \"Open\")\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"OpenMA_7MA_30_UpperQuantile\"], 'r--', label = \"OpenMA_7MA_30_UpperQuantile\")\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"OpenMA_7MA_30_LowerQuantile\"], 'g--', label = \"OpenMA_7MA_30_LowerQuantile\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Open Price')\n",
    "\taxs[i].label_outer()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t# ax2.bar(stockData[\"Date\"], stockData[\"Open_IsJumping\"], label=\"open jump\")\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\n",
    "\tax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7\"], label = \"VolumeMA_7\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7MA_30_UpperQuantile\"], 'r--', label = \"VolumeMA_7MA_30_UpperQuantile\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7MA_30_LowerQuantile\"], 'g--', label = \"VolumeMA_7MA_30_LowerQuantile\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Volume')\n",
    "\taxs[i].label_outer()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t\n",
    "\t# ax2.bar(stockData[\"Date\"], stockData[\"Volume_IsJumping\"], label=\"volume jump\")\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\n",
    "\tax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i])\n",
    "\t\n",
    "\tgrouped = tweetData.groupby([\"Date\"]).size().reset_index(name='counts')\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].scatter(grouped[\"Date\"], grouped[\"counts\"], label = \"Volume\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Volume')\n",
    "\taxs[i].label_outer()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\t\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "positive sentiment: compound score >= 0.05\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "negative sentiment: compound score <= -0.05\n",
    "NOTE: The compound score is the one most commonly used for sentiment analysis by most researchers, including the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Sentiment / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i])\n",
    "\t\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\n",
    "\tavgSentiment = tweetData.groupby(\"Date\")['Sentiment'].mean().reset_index(name='Sentiment')\n",
    "\tCreateMovingAverage(avgSentiment, \"Sentiment\", 7)\n",
    "\taxs[i].plot(avgSentiment[\"Date\"], avgSentiment[\"SentimentMA_7\"], label = \"Sentiment MA_7\")\n",
    "\n",
    "\t\n",
    "\taxs[i].scatter(tweetData[\"Date\"], tweetData[\"Sentiment\"], label = \"Sentiment\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Sentiment')\n",
    "\taxs[i].label_outer()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i], isPublicOpinion=True)\n",
    "\t\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 7)\n",
    "\n",
    "\t\n",
    "\t# axs[i].scatter(tweetData[\"Date\"], tweetData[\"avgSentiment\"], label = \"avgSentiment\")\n",
    "\taxs[i].plot(tweetData[\"Date\"], tweetData[\"avgSentimentMA_3\"], label = \"avgSentiment MA_7\")\n",
    "\n",
    "\taxs[i].set(xlabel='Date', ylabel='avgSentiment')\n",
    "\taxs[i].label_outer()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\t# stockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"Stock jump\")\n",
    "\tax2.bar(tweetData[\"Date\"], tweetData[\"avgSentiment_IsJumping\"], label=\"Tweet jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDaysBetweenTweetAndStock(tweetDates, stockDates):\n",
    "\ttweetDates = sorted(tweetDates)\n",
    "\tstockDates = sorted(stockDates)\n",
    "\n",
    "\tdaysToJumpList = []\n",
    "\tlastStockIndex = 0\n",
    "\tfor tweetDate in tweetDates:\n",
    "\n",
    "\t\tfor i in range(lastStockIndex, len(stockDates)):\n",
    "\t\t\t\n",
    "\t\t\tstockDate = stockDates[i]\n",
    "\t\t\tdelta = (stockDate - tweetDate).days\n",
    "\t\t\tif delta >= 0:\n",
    "\t\t\t\tlastStockIndex = i\n",
    "\t\t\t\tdaysToJumpList.append(delta)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\treturn daysToJumpList\n",
    "\n",
    "def AvgDaysBetweenJumps(dates):\n",
    "\tdates = sorted(dates)\n",
    "\n",
    "\tdeltas = []\n",
    "\tfor i in range(len(dates) - 1):\n",
    "\t\tdeltas.append((dates[i+1] - dates[i]).days)\n",
    "\treturn sum(deltas) / len(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of days between each tweeter jump and stock jump\n",
    "\n",
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Days between tweet jumps and stock jumps\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tcompanyData = companiesToReview[i]\n",
    "\taxs[i].set_title(companyData[\"StockName\"])\n",
    "\n",
    "\ttweetJumpDates = companyData[\"TweetJumpDates\"]\n",
    "\tstockJumpDates = companyData[\"StockJumpDates\"]\n",
    "\tdaysToJumpList = GetDaysBetweenTweetAndStock(tweetJumpDates, stockJumpDates)\n",
    "\n",
    "\tavgGapBetweenTweetJumps = AvgDaysBetweenJumps(tweetJumpDates)\n",
    "\tavgGapBetweenStockJumps = AvgDaysBetweenJumps(stockJumpDates)\n",
    "\t\n",
    "\taxs[i].hist(daysToJumpList, 50)\n",
    "\n",
    "\taxs[i].hist([0, avgGapBetweenTweetJumps, max(daysToJumpList)], 100)\n",
    "\taxs[i].hist([0, avgGapBetweenStockJumps, max(daysToJumpList)], 100)\n",
    "\n",
    "\n",
    "\taxs[i].set(xlabel='Number of days between jumps', ylabel='Count')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].xaxis.set_ticks(np.arange(0, 300, 7))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e03891b53087abc58e95159b2c70e6ab6b812f7c74a522802c5c9765390d0ed"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
