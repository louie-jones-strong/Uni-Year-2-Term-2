{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mid term Report title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import yfinance as yf\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline  \n",
    "# using the style for the plot\n",
    "plt.style.use('dark_background')\n",
    "# set plot size\n",
    "plt.rcParams['figure.figsize'] = [30, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scope Limits\n",
    "StartTime = \"2012-05-01\"\n",
    "EndTime = \"2022-05-01\"\n",
    "TweetLimit = 100\n",
    "\n",
    "# scope of a jump\n",
    "JumpTriggerRatio = 1\n",
    "JumpTriggerPercentage = 5\n",
    "MinDaysBetweenJumps = 7\n",
    "\n",
    "\n",
    "# caching folders\n",
    "TweetCacheFolder = \"CachedTweets\"\n",
    "StockCacheFolder = \"CachedStockData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what users do we care about\n",
    "\n",
    "# names from\n",
    "# https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts\n",
    "\n",
    "with open(\"TwitterUserNames.json\", \"r\") as file:\n",
    "\tusernames = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweets from users\n",
    "def GetTweetsFromUser(username):\n",
    "\n",
    "\tif not os.path.isdir(TweetCacheFolder):\n",
    "\t\tos.mkdir(TweetCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(TweetCacheFolder, \"tweets_\" + username + \".csv\")\n",
    "\n",
    "\t# cache exists\n",
    "\tif os.path.exists(cachePath):\n",
    "\t\tdf = pd.read_csv(cachePath)\n",
    "\n",
    "\telse:\n",
    "\t\tquery = \"(from:\" + username + \") since:\" + StartTime + \" until:\" + EndTime\n",
    "\t\t\n",
    "\t\ttweets = []\n",
    "\t\tfor tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "\t\t\tif len(tweets) == TweetLimit:\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\t# print(tweet.json())\n",
    "\t\t\t\t#break\n",
    "\t\t\t\ttweets.append([tweet.date, tweet.user.username, tweet.content, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.id])\n",
    "\t\t\n",
    "\t\tdf = pd.DataFrame(tweets, columns=['Date', 'User', 'Content', 'ReplyCount', 'RetweetCount', 'LikeCount', 'QuoteCount', 'TweetID'])\n",
    "\n",
    "\t\t# to save to csv as a cache\n",
    "\t\tdf.to_csv(cachePath)\n",
    "\t\n",
    "\n",
    "\treturn df\n",
    "\n",
    "tweetsDf = GetTweetsFromUser(usernames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for username in usernames:\n",
    "\ttweetsDf = GetTweetsFromUser(username)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stocks to track\n",
    "with open(\"StockTags_SnP.json\", \"r\") as file:\n",
    "\tstockTags = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadStockData(stockTag):\n",
    "\n",
    "\tif not os.path.isdir(StockCacheFolder):\n",
    "\t\tos.mkdir(StockCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(StockCacheFolder, \"Stock_\" + stockTag + \".csv\")\n",
    "\n",
    "\t# cache exists\n",
    "\tif os.path.exists(cachePath):\n",
    "\t\tstockDataDf = pd.read_csv(cachePath)\n",
    "\n",
    "\telse:\n",
    "\t\tstockDataDf = yf.download(stockTag, start=StartTime, end=EndTime, progress=False)\n",
    "\n",
    "\t\t#save data to local cache file\n",
    "\t\tstockDataDf.to_csv(cachePath)\n",
    "\n",
    "\treturn stockDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stockTag in stockTags:\n",
    "\tLoadStockData(stockTag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to review the data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMovingAverage(df, columnToAverage, averageLength):\n",
    "\n",
    "\tmovingAverage = []\n",
    "\tmovingAverageUpperQuantile = []\n",
    "\tmovingAverageLowerQuantile = []\n",
    "\n",
    "\trows = df.loc[:,columnToAverage]\n",
    "\tfor i in range(len(rows)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < averageLength:\n",
    "\t\t\twindow = [0]\n",
    "\t\telse:\n",
    "\t\t\twindow = rows[i - averageLength : i]\n",
    "\n",
    "\t\t# calculate moving avg\n",
    "\t\twindowAverage = round(sum(window) / averageLength, 2)\n",
    "\t\tmovingAverage.append(windowAverage)\n",
    "\n",
    "\t\t# calculate UpperQuantile\n",
    "\t\tupperQuantile = np.quantile(window, 0.9)\n",
    "\t\tmovingAverageUpperQuantile.append(upperQuantile)\n",
    "\n",
    "\t\t# calculate LowerQuantile\n",
    "\t\tlowerQuantile = np.quantile(window, 0.1)\n",
    "\t\tmovingAverageLowerQuantile.append(lowerQuantile)\n",
    "\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength), movingAverage, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_UpperQuantile\", movingAverageUpperQuantile, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_LowerQuantile\", movingAverageLowerQuantile, allow_duplicates=True)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectValueJumping(df, column, lastValueColumn, currentValueColumn):\n",
    "\tisValueJumping = []\n",
    "\n",
    "\tfor i in range(len(df)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < 45: #skip the first few days as the moving averages take a few days to settle\n",
    "\t\t\tisJump = False\n",
    "\t\telse:\n",
    "\t\t\tupperQuantile = df[column + \"MA_7MA_30_UpperQuantile\"][i]\n",
    "\t\t\tlowerQuantile = df[column + \"MA_7MA_30_LowerQuantile\"][i]\n",
    "\t\t\tlastValue = df[lastValueColumn][i]\n",
    "\t\t\tcurrentValue = df[currentValueColumn][i]\n",
    "\n",
    "\t\t\tinterQuantileRange = upperQuantile - lowerQuantile\n",
    "\t\t\ttriggerRange = JumpTriggerRatio * interQuantileRange\n",
    "\n",
    "\t\t\tmeantRatio = currentValue > upperQuantile + triggerRange or currentValue < lowerQuantile - triggerRange\n",
    "\n",
    "\t\t\tmeantPercentage = False\n",
    "\t\t\tif lastValue != 0:\n",
    "\t\t\t\tmeantPercentage = abs(lastValue - currentValue) / lastValue > JumpTriggerPercentage / 100\n",
    "\t\t\t\n",
    "\t\t\tisJump = meantRatio and meantPercentage\n",
    "\n",
    "\t\tisValueJumping.append(isJump)\n",
    "\n",
    "\tdf.insert(len(df.columns), column + \"_IsJumping\", isValueJumping, allow_duplicates=True)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def CreateStockTrendData(stockData):\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 7)\n",
    "\tCreateMovingAverage(stockData, \"VolumeMA_7\", 30)\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Open\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Open\", 7)\n",
    "\tCreateMovingAverage(stockData, \"OpenMA_7\", 30)\n",
    "\n",
    "\tDetectJumps(stockData)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def DetectJumps(stockData):\n",
    "\n",
    "\tDetectValueJumping(stockData, \"Volume\", \"VolumeMA_7\", \"VolumeMA_3\")\n",
    "\tDetectValueJumping(stockData, \"Open\", \"OpenMA_3\", \"Open\")\n",
    "\n",
    "\t\n",
    "\tbothJumped = stockData[\"Volume_IsJumping\"] * stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Both_IsJumping\", bothJumped, allow_duplicates=True)\n",
    "\n",
    "\teitherJumped = stockData[\"Volume_IsJumping\"] + stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Either_IsJumping\", eitherJumped, allow_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stockTag in stockTags:\n",
    "\tstockData = LoadStockData(stockTag)\n",
    "\tCreateStockTrendData(stockData)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCharts = 6\n",
    "\n",
    "fig, axs = plt.subplots(numCharts, sharex=True)\n",
    "fig.suptitle(\"Open Price / Time\")\n",
    "\n",
    "for i in range(numCharts):\n",
    "\tstockData = LoadStockData(stockTags[i])\n",
    "\tCreateStockTrendData(stockData)\n",
    "\n",
    "\taxs[i].set_title(stockTags[i])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"Open\"], label = \"Open\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"OpenMA_7MA_30_UpperQuantile\"], 'r--', label = \"OpenMA_7MA_30_UpperQuantile\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"OpenMA_7MA_30_LowerQuantile\"], 'g--', label = \"OpenMA_7MA_30_LowerQuantile\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Open Price')\n",
    "\taxs[i].label_outer()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t# ax2.bar(stockData[\"Date\"], stockData[\"Open_IsJumping\"], label=\"open jump\")\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\n",
    "\tax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCharts = 6\n",
    "\n",
    "fig, axs = plt.subplots(numCharts, sharex=True)\n",
    "fig.suptitle(\"Volume / Time\")\n",
    "\n",
    "for i in range(numCharts):\n",
    "\tstockData = LoadStockData(stockTags[i])\n",
    "\tCreateStockTrendData(stockData)\n",
    "\n",
    "\taxs[i].set_title(stockTags[i])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7\"], label = \"VolumeMA_7\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7MA_30_UpperQuantile\"], 'r--', label = \"VolumeMA_7MA_30_UpperQuantile\")\n",
    "\t# axs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7MA_30_LowerQuantile\"], 'g--', label = \"VolumeMA_7MA_30_LowerQuantile\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Volume')\n",
    "\taxs[i].label_outer()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t\n",
    "\t# ax2.bar(stockData[\"Date\"], stockData[\"Volume_IsJumping\"], label=\"volume jump\")\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], label=\"both jump\")\n",
    "\n",
    "\tax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to get the dates that we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStockJumpsDates(stockData, jumpColumn):\n",
    "\n",
    "\tdates = []\n",
    "\tdaysSinceLastJump = MinDaysBetweenJumps\n",
    "\n",
    "\tfor i in range(len(stockData)):\n",
    "\t\tdaysSinceLastJump += 1\n",
    "\n",
    "\t\tisJump = stockData[jumpColumn][i]\n",
    "\n",
    "\t\tif isJump and daysSinceLastJump >= MinDaysBetweenJumps:\n",
    "\t\t\tdates.append(stockData[\"Date\"][i])\n",
    "\t\t\tdaysSinceLastJump = 0\n",
    "\n",
    "\treturn dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many jump dates are there between all the stocks\n",
    "\n",
    "bothSet = set()\n",
    "volumeSet = set()\n",
    "openSet = set()\n",
    "\n",
    "for stockTag in stockTags:\n",
    "\tstockData = LoadStockData(stockTag)\n",
    "\tCreateStockTrendData(stockData)\n",
    "\n",
    "\tbothSet.update(GetStockJumpsDates(stockData, \"Both_IsJumping\"))\n",
    "\tvolumeSet.update(GetStockJumpsDates(stockData, \"Volume_IsJumping\"))\n",
    "\topenSet.update(GetStockJumpsDates(stockData, \"Open_IsJumping\"))\n",
    "\n",
    "\n",
    "print(\"number of Open jumps: \" + str(len(openSet)))\n",
    "print(\"number of Volume jumps: \" + str(len(volumeSet)))\n",
    "print(\"number of Both jumps: \" + str(len(bothSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stockTag in stockTags:\n",
    "\tstockData = LoadStockData(stockTag)\n",
    "\tCreateStockTrendData(stockData)\n",
    "\n",
    "\tdates = GetStockJumpsDates(stockData, \"Both_IsJumping\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e03891b53087abc58e95159b2c70e6ab6b812f7c74a522802c5c9765390d0ed"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
