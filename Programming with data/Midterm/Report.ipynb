{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Twitter Be Used To Predict Changes In a Stock value\n",
    "\n",
    "## 1 An Introduction To The Problem / field\n",
    "There is of course a lot of interest in tring to find any sort of information that can help to predict the outcome of the stock markets. As there is a lot of money being traded in the stock markets every day, any edge that can be gain in predicting when and in what direction a stock will change can enable large profits.\n",
    "\n",
    "Due to the large values of such information there has been a lot of research already and continuing to be done in the field, however most of it hasn't be made public due to the arms race between top stock traders.\n",
    "\n",
    "Company's stocks are valued based on how much the company is currently worth and how much the company is expected to grow over the coming months and years.\n",
    "\n",
    "Therefore if you are able to tell when the population's feelings/confidence in a company's value/grow increases or decreases. Then you would expect the stock market value of the to follow that same trend.\n",
    "\n",
    "\n",
    "\n",
    "## 2 What Data Am I Going To Use\n",
    "I am planing to curate a list popular companies from the snp 500, this list will also contain the twitter search queries needed to down load the tweets from the company.\n",
    "I am also going to look at the sentiment and volume of tweets from the public about the company. \n",
    "I will do this by searching for tweets use that hash tags\n",
    "\n",
    "### 2.1 To companies I will review are:\n",
    " - Tesla\n",
    " - Apple\n",
    " - Microsoft\n",
    " - Amazon\n",
    " - Alphabet\n",
    " - Meta\n",
    " - Twitter\n",
    "\n",
    "\n",
    "### 2.2 Stock market\n",
    "I am going to collect the stock data from yahoo finance library, this greatly increases the easy for downloading this data\n",
    "\n",
    "#### 2.2.1 Ethical Impact\n",
    "##### **Collection and distribution of collected data?**\n",
    "All of this data is already public and easy to review and collect, there for this project doesn't make it any easier for anyone\n",
    "\n",
    "##### **Impact of Findings?**\n",
    "\n",
    "\n",
    "##### **Is the Data Personally Identifiable?**\n",
    "N/A\n",
    "\n",
    "##### **biases in the data?**\n",
    "I have only selected companies that I know of and they are all Tech based companies.\n",
    "Each of these Companies are know success as they have been selected for the snp 500.\n",
    "\n",
    "Due to this selection process bias I expect these companies will be more linked to twitter then other Companies.\n",
    "\n",
    "Therefore this cannot be used a confirmation for other stocks as it's isn't representative of all stocks\n",
    "\n",
    "\n",
    "### 2.3 Twitter\n",
    "I original looked at using Twitter's API to collect all the of tweets,\n",
    "however it was harder to use and it also added a rate limit on hte number of tweets that I could collect.\n",
    "I realised that the limits would be too low for what I was wanting to do. But I found the snscrape library, this offers a much easier experience, with no cap and also allowing to easily expand the scope to other social networks as well if I want.\n",
    "\n",
    "#### 2.3.1 Ethical Impact\n",
    "##### **Collection and distribution of collected data?**\n",
    "\n",
    "\n",
    "##### **Impact of Findings?**\n",
    "If this project is found to show that tweets can provide signals to the a stocks future performance, then this could encourage further work into these tweets.\n",
    "I cannot guarantee how others will review this data, but they could end up using it in non ethical ways.\n",
    "Also due to the amount of money in this area, a lot of money could be put in to extra data collection that might be Personally Identifiable\n",
    "\n",
    "##### **Is the Data Personally Identifiable?**\n",
    "The raw data that is collected is Personally Identifiable.\n",
    "However always review the aggregated values from the tweets never single tweets.\n",
    "\n",
    "\n",
    "##### **biases in the data?**\n",
    "The public sentiment about the companies are collected from tweets that contain hash tags that i felt were relevant.\n",
    "There could also be a lot of tweets about the companies that didn't include hashtags. so there is a selection bias to users/ tweets that meet that criteria.\n",
    "\n",
    "As some of the companies i am reviewing are social networks, there is likely far more bias, due to the data only being collected from twitter.\n",
    "I would expect twitter users to be more positive about twitter and less so about facebook(meta).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "\n",
    "#yahoo finance to down load the relevent stock market data\n",
    "import yfinance as yf\n",
    "\n",
    "# social network scrape to download the tweets from twitter\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#libraries for cleaning the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#libraries for file read and writing\n",
    "import json\n",
    "import os\n",
    "\n",
    "#libraries for parsing and converting date time formats\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "#libraries for analyzing the sentiment of the tweets\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SentimentAnalyzer\n",
    "sentimentAnalyzer = SentimentAnalyzer()\n",
    "\n",
    "#libraries for plotting data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Show all matplotlib graphs inline\n",
    "%matplotlib inline  \n",
    "# using the style for the plot\n",
    "plt.style.use('dark_background')\n",
    "# set plot size\n",
    "plt.rcParams['figure.figsize'] = [30, 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Limitations To Project Scope\n",
    "This is is a very large field that can be researched for a life time so I have to limit the scope of the project, as I have a 4 week time limit.\n",
    "\n",
    "I did experiment with a wider scope, but the extended time needed for computation reduced my ability to iterate.\n",
    "I wanted to focus on a few companies at a deeper evaluation instead of little to no value of evaluation spread over more stocks.\n",
    "\n",
    "#### 3.1 Stock Data:\n",
    " - I am not going to look at all trades happening in each day only the open price and volume\n",
    " - I am only going to look at the last 10 years of data\n",
    "\n",
    "#### 3.2 Twitter Data:\n",
    " - First of all I will review the tweets that are sent from the company or it's CEO\n",
    " - then look at all tweets about the company\n",
    "  - this will be limited to the when the company's hashtags are used in tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scope Limits\n",
    "\n",
    "# Limit the time range that I am going to review\n",
    "StartTime = \"2012-05-01\"\n",
    "EndTime = \"2022-05-01\"\n",
    "\n",
    "# what counts as a jump in values \n",
    "MinDaysBetweenJumps = 7\n",
    "\n",
    "StockJumpTriggerRatio = 0.5\n",
    "StockJumpTriggerPercentage = 1\n",
    "\n",
    "# twitter's data is more volatile so needs a larger threshold\n",
    "TwitterJumpTriggerRatio = 1.25\n",
    "TwitterJumpTriggerPercentage = 7.5\n",
    "\n",
    "#Where to cache data\n",
    "TweetCacheFolder_Small = \"CachedTweets_Small\"\n",
    "TweetCacheFolder_Large = \"CachedTweets_Large\"\n",
    "StockCacheFolder = \"CachedStockData\"\n",
    "\n",
    "TweetCacheDict = {}\n",
    "StockCacheDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what users do we care about\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "\tcompaniesToReview = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StrToDate(date):\n",
    "\ttry:\n",
    "\t\tdate = parser.parse(date)\n",
    "\texcept:\n",
    "\t\traise Exception(\"invalid date: \" + str(date) + \" Of type: \" + str(type(date)))\n",
    "\t\n",
    "\t\n",
    "\tdate = datetime(date.year, date.month, date.day)\n",
    "\treturn date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTweetSentiment(text):\n",
    "\tscores = sentimentAnalyzer.polarity_scores(text)\n",
    "\treturn scores[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTweetsFromAboutCompany(companyData, isPublicOpinion=False):\n",
    "\n",
    "\ttweetCacheFolder = TweetCacheFolder_Small\n",
    "\tdictKey = companyData[\"StockName\"]\n",
    "\n",
    "\tif isPublicOpinion: \n",
    "\t\ttweetCacheFolder =TweetCacheFolder_Large\n",
    "\t\tdictKey += \"_isPublicOpinion\"\n",
    "\n",
    "\tif not os.path.isdir(tweetCacheFolder):\n",
    "\t\tos.mkdir(tweetCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(tweetCacheFolder, \"tweets_\" + companyData[\"StockName\"] + \".csv\")\n",
    "\n",
    "\t#dict cache exists\n",
    "\tif dictKey in TweetCacheDict:\n",
    "\t\tdf = TweetCacheDict[dictKey]\n",
    "\n",
    "\t# file cache exists\n",
    "\telse:\n",
    "\t\tif os.path.exists(cachePath):\n",
    "\t\t\tdf = pd.read_csv(cachePath)\n",
    "\n",
    "\t\t\t\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# build the query\n",
    "\t\t\tquery = \"(\"\n",
    "\n",
    "\t\t\ttwitterUsernames = companyData[\"TwitterUsernames\"]\n",
    "\t\t\ttwitterHashTags = companyData[\"TwitterHashTags\"]\n",
    "\t\t\ttwitterPhrases = companyData[\"TwitterPhrases\"]\n",
    "\n",
    "\t\t\tfor i in range(len(twitterUsernames)):\n",
    "\t\t\t\tquery += \"from:\" + twitterUsernames[i]\n",
    "\t\t\t\tif i < len(twitterUsernames) - 1:\n",
    "\t\t\t\t\tquery += \" OR \"\n",
    "\n",
    "\t\t\tif isPublicOpinion:\n",
    "\t\t\t\tquery += \") OR (\"\n",
    "\n",
    "\t\t\t\tfor i in range(len(twitterHashTags)):\n",
    "\t\t\t\t\tquery += twitterHashTags[i]\n",
    "\t\t\t\t\tif i < len(twitterHashTags) - 1:\n",
    "\t\t\t\t\t\tquery += \" OR \"\n",
    "\n",
    "\t\t\tquery += \") since:\" + StartTime + \" until:\" + EndTime + \" -filter:links -filter:replies\"\n",
    "\t\t\t\n",
    "\t\t\t# get tweets that match the query\n",
    "\t\t\ttweets = []\n",
    "\t\t\tfor tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\t\t\t\tsentiment = GetTweetSentiment(tweet.content)\n",
    "\t\t\t\ttweets.append([tweet.date, tweet.user.username, tweet.content, tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.id, sentiment])\n",
    "\n",
    "\t\t\tdf = pd.DataFrame(tweets, columns=['Date', 'User', 'Content', 'ReplyCount', 'RetweetCount', 'LikeCount', 'QuoteCount', 'TweetID', 'Sentiment'])\n",
    "\n",
    "\t\t\t# to save to csv as a cache\n",
    "\t\t\tdf.to_csv(cachePath)\n",
    "\n",
    "\t\tfor i in range(len(df)):\n",
    "\t\t\tdf[\"Date\"][i] = StrToDate(df[\"Date\"][i])\n",
    "\n",
    "\t\tdf = df.sort_values(by=['Date'])\n",
    "\t\tTweetCacheDict[dictKey] = df\n",
    "\t\n",
    "\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadStockData(stockTag):\n",
    "\n",
    "\tif not os.path.isdir(StockCacheFolder):\n",
    "\t\tos.mkdir(StockCacheFolder)\n",
    "\n",
    "\tcachePath = os.path.join(StockCacheFolder, \"Stock_\" + stockTag + \".csv\")\n",
    "\n",
    "\t#dict cache exists\n",
    "\tif stockTag in TweetCacheDict:\n",
    "\t\tstockDataDf = TweetCacheDict[stockTag]\n",
    "\n",
    "\t# file cache exists\n",
    "\telse:\n",
    "\t\tif os.path.exists(cachePath):\n",
    "\t\t\tstockDataDf = pd.read_csv(cachePath)\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tstockDataDf = yf.download(stockTag, start=StartTime, end=EndTime, progress=False)\n",
    "\n",
    "\t\t\t#save data to local cache file\n",
    "\t\t\tstockDataDf.to_csv(cachePath)\n",
    "\n",
    "\n",
    "\t\tfor i in range(len(stockDataDf)):\n",
    "\t\t\tstockDataDf[\"Date\"][i] = StrToDate(stockDataDf[\"Date\"][i])\n",
    "\n",
    "\t\tstockDataDf = stockDataDf.sort_values(by=['Date'])\n",
    "\t\tTweetCacheDict[stockTag] = stockDataDf\n",
    "\n",
    "\t\n",
    "\treturn stockDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMovingAverage(df, columnToAverage, averageLength):\n",
    "\n",
    "\tmovingAverage = []\n",
    "\tmovingAverageUpperQuantile = []\n",
    "\tmovingAverageLowerQuantile = []\n",
    "\n",
    "\trows = df.loc[:,columnToAverage]\n",
    "\tfor i in range(len(rows)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < averageLength:\n",
    "\t\t\twindow = [0]\n",
    "\t\telse:\n",
    "\t\t\twindow = rows[i - averageLength : i]\n",
    "\n",
    "\t\t# calculate moving avg\n",
    "\t\twindowAverage = round(sum(window) / averageLength, 2)\n",
    "\t\tmovingAverage.append(windowAverage)\n",
    "\n",
    "\t\t# calculate UpperQuantile\n",
    "\t\tupperQuantile = np.quantile(window, 0.9)\n",
    "\t\tmovingAverageUpperQuantile.append(upperQuantile)\n",
    "\n",
    "\t\t# calculate LowerQuantile\n",
    "\t\tlowerQuantile = np.quantile(window, 0.1)\n",
    "\t\tmovingAverageLowerQuantile.append(lowerQuantile)\n",
    "\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength), movingAverage, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_UpperQuantile\", movingAverageUpperQuantile, allow_duplicates=True)\n",
    "\tdf.insert(len(df.columns), columnToAverage + \"MA_\" + str(averageLength) + \"_LowerQuantile\", movingAverageLowerQuantile, allow_duplicates=True)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectValueJumping(df, column, lastValueColumn, currentValueColumn, jumpTriggerRatio, jumpTriggerPercentage):\n",
    "\tisValueJumping = []\n",
    "\n",
    "\tfor i in range(len(df)):\n",
    "\n",
    "\t\t# get window\n",
    "\t\tif i < 45: #skip the first few days as the moving averages take a few days to settle\n",
    "\t\t\tisJump = False\n",
    "\t\telse:\n",
    "\t\t\tupperQuantile = df[column + \"MA_7MA_30_UpperQuantile\"][i]\n",
    "\t\t\tlowerQuantile = df[column + \"MA_7MA_30_LowerQuantile\"][i]\n",
    "\t\t\tlastValue = df[lastValueColumn][i]\n",
    "\t\t\tcurrentValue = df[currentValueColumn][i]\n",
    "\n",
    "\t\t\tinterQuantileRange = upperQuantile - lowerQuantile\n",
    "\t\t\ttriggerRange = jumpTriggerRatio * interQuantileRange\n",
    "\n",
    "\t\t\tmeantRatio = currentValue > upperQuantile + triggerRange or currentValue < lowerQuantile - triggerRange\n",
    "\n",
    "\t\t\tmeantPercentage = False\n",
    "\t\t\tif lastValue != 0:\n",
    "\t\t\t\tmeantPercentage = abs(lastValue - currentValue) / lastValue > jumpTriggerPercentage / 100\n",
    "\t\t\t\n",
    "\t\t\tisJump = meantRatio and meantPercentage\n",
    "\n",
    "\t\tisValueJumping.append(isJump)\n",
    "\n",
    "\tdf.insert(len(df.columns), column + \"_IsJumping\", isValueJumping, allow_duplicates=True)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def CreateStockTrendData(stockData):\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Volume\", 7)\n",
    "\tCreateMovingAverage(stockData, \"VolumeMA_7\", 30)\n",
    "\n",
    "\tCreateMovingAverage(stockData, \"Open\", 3)\n",
    "\tCreateMovingAverage(stockData, \"Open\", 7)\n",
    "\tCreateMovingAverage(stockData, \"OpenMA_7\", 30)\n",
    "\n",
    "\tDetectStockJumps(stockData)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def DetectStockJumps(stockData):\n",
    "\n",
    "\tDetectValueJumping(stockData, \"Volume\", \"VolumeMA_7\", \"VolumeMA_3\",\n",
    "\t\tStockJumpTriggerRatio, StockJumpTriggerPercentage)\n",
    "\tDetectValueJumping(stockData, \"Open\", \"OpenMA_3\", \"Open\",\n",
    "\t\tStockJumpTriggerRatio, StockJumpTriggerPercentage)\n",
    "\n",
    "\t\n",
    "\tbothJumped = stockData[\"Volume_IsJumping\"] * stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Both_IsJumping\", bothJumped, allow_duplicates=True)\n",
    "\n",
    "\teitherJumped = stockData[\"Volume_IsJumping\"] + stockData[\"Open_IsJumping\"]\n",
    "\tstockData.insert(len(stockData.columns), \"Either_IsJumping\", eitherJumped, allow_duplicates=True)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def DetectTweetJumps(tweetData):\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 3)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 7)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 30)\n",
    "\tCreateMovingAverage(tweetData, \"avgSentimentMA_7\", 30)\n",
    "\n",
    "\tDetectValueJumping(tweetData, \"avgSentiment\", \"avgSentimentMA_7MA_30\", \"avgSentimentMA_7\",\n",
    "\t\tTwitterJumpTriggerRatio, TwitterJumpTriggerPercentage)\n",
    "\n",
    "\treturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dates that values jump\n",
    "def GetValueJumpsDates(stockData, jumpColumn):\n",
    "\n",
    "\tdates = []\n",
    "\tdaysSinceLastJump = MinDaysBetweenJumps\n",
    "\n",
    "\tfor i in range(len(stockData)):\n",
    "\t\tdaysSinceLastJump += 1\n",
    "\n",
    "\t\tisJump = stockData[jumpColumn][i]\n",
    "\n",
    "\t\tif isJump and daysSinceLastJump >= MinDaysBetweenJumps:\n",
    "\t\t\tdates.append(stockData[\"Date\"][i])\n",
    "\t\t\tdaysSinceLastJump = 0\n",
    "\n",
    "\treturn dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#down load all needed data to local cache\n",
    "# then clean up the data\n",
    "for companyData in companiesToReview:\n",
    "\tGetTweetsFromAboutCompany(companyData)\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companyData, isPublicOpinion=True)\n",
    "\n",
    "\tDetectTweetJumps(tweetData)\n",
    "\n",
    "\tcompanyData[\"TweetJumpDates\"] = GetValueJumpsDates(tweetData, \"avgSentiment_IsJumping\")\n",
    "\t\n",
    "\n",
    "\tstockData = LoadStockData(companyData[\"Tag\"])\n",
    "\tCreateStockTrendData(stockData)\n",
    "\tcompanyData[\"StockJumpDates\"] = GetValueJumpsDates(stockData, \"Both_IsJumping\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Check the data is valid\n",
    "\n",
    "Now that the data is all downloaded, we need to check that it looks correct\n",
    "\n",
    "### 4.1 Checking The Stock Data\n",
    "I am going to create two plots each of stock one of the open price over time and one of the volume over time\n",
    "This will also show an overlay of where the predicting jumps in the stock price is happening\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Stock Open Price / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"Open\"], label = \"Open Price\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Open Price')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].legend()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t# ax2.bar(stockData[\"Date\"], stockData[\"Open_IsJumping\"], label=\"open jump\")\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Stock Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].plot(stockData[\"Date\"], stockData[\"VolumeMA_7\"], label = \"Volume 7 day avg\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Volume')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].legend()\n",
    "\n",
    "\tax2 = axs[i].twinx()\n",
    "\t\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Checking The Twitter Data\n",
    "\n",
    "\n",
    "#### How Sentiment is found\n",
    "\n",
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "positive sentiment: compound score >= 0.05\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "negative sentiment: compound score <= -0.05\n",
    "NOTE: The compound score is the one most commonly used for sentiment analysis by most researchers, including the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Company's Tweet Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i])\n",
    "\t\n",
    "\tgrouped = tweetData.groupby([\"Date\"]).size().reset_index(name='counts')\n",
    "\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\taxs[i].plot(grouped[\"Date\"], grouped[\"counts\"], label = \"Tweet Volume\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Volume')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].legend()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\t\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Company's Tweet Sentiment / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i])\n",
    "\t\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\n",
    "\tavgSentiment = tweetData.groupby(\"Date\")['Sentiment'].mean().reset_index(name='Sentiment')\n",
    "\tCreateMovingAverage(avgSentiment, \"Sentiment\", 7)\n",
    "\taxs[i].plot(avgSentiment[\"Date\"], avgSentiment[\"SentimentMA_7\"], \"r--\", label = \"Sentiment 7 day avg\")\n",
    "\n",
    "\t\n",
    "\taxs[i].scatter(tweetData[\"Date\"], tweetData[\"Sentiment\"], label = \"Sentiment\")\n",
    "\t\n",
    "\taxs[i].set(xlabel='Date', ylabel='Sentiment')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].legend()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Public Opinion Tweet Volume / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i], isPublicOpinion=True)\n",
    "\t\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\n",
    "\taxs[i].plot(tweetData[\"Date\"], tweetData[\"countTweets\"])\n",
    "\n",
    "\taxs[i].set(xlabel='Date', ylabel='avgSentiment')\n",
    "\taxs[i].label_outer()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\tax2.bar(tweetData[\"Date\"], tweetData[\"avgSentiment_IsJumping\"], color=['cyan'], label=\"Tweet jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Public Opinion Tweet Sentiment / Time\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\n",
    "\ttweetData = GetTweetsFromAboutCompany(companiesToReview[i], isPublicOpinion=True)\n",
    "\t\n",
    "\taxs[i].set_title(companiesToReview[i][\"StockName\"])\n",
    "\n",
    "\tCreateMovingAverage(tweetData, \"avgSentiment\", 7)\n",
    "\n",
    "\t\n",
    "\taxs[i].plot(tweetData[\"Date\"], tweetData[\"avgSentimentMA_3\"], label = \"avgSentiment MA_7\")\n",
    "\n",
    "\taxs[i].set(xlabel='Date', ylabel='avgSentiment')\n",
    "\taxs[i].label_outer()\n",
    "\t\n",
    "\tax2 = axs[i].twinx()\n",
    "\tstockData = LoadStockData(companiesToReview[i][\"Tag\"])\n",
    "\n",
    "\tax2.bar(stockData[\"Date\"], stockData[\"Both_IsJumping\"], color=['green'], label=\"Stock jump\")\n",
    "\tax2.bar(tweetData[\"Date\"], tweetData[\"avgSentiment_IsJumping\"], color=['cyan'], label=\"Tweet jump\")\n",
    "\tax2.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDaysBetweenTweetAndStock(tweetDates, stockDates):\n",
    "\ttweetDates = sorted(tweetDates)\n",
    "\tstockDates = sorted(stockDates)\n",
    "\n",
    "\tdaysToJumpList = []\n",
    "\tlastStockIndex = 0\n",
    "\tfor tweetDate in tweetDates:\n",
    "\n",
    "\t\tfor i in range(lastStockIndex, len(stockDates)):\n",
    "\t\t\t\n",
    "\t\t\tstockDate = stockDates[i]\n",
    "\t\t\tdelta = (stockDate - tweetDate).days\n",
    "\t\t\tif delta >= 0:\n",
    "\t\t\t\tlastStockIndex = i\n",
    "\t\t\t\tdaysToJumpList.append(delta)\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\n",
    "\treturn daysToJumpList\n",
    "\n",
    "def AvgDaysBetweenJumps(dates):\n",
    "\tdates = sorted(dates)\n",
    "\n",
    "\tdeltas = []\n",
    "\tfor i in range(len(dates) - 1):\n",
    "\t\tdeltas.append((dates[i+1] - dates[i]).days)\n",
    "\treturn sum(deltas) / len(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of days between each tweeter jump and stock jump\n",
    "\n",
    "fig, axs = plt.subplots(len(companiesToReview), sharex=True)\n",
    "fig.suptitle(\"Days between tweet jumps and stock jumps\")\n",
    "\n",
    "for i in range(len(companiesToReview)):\n",
    "\tcompanyData = companiesToReview[i]\n",
    "\taxs[i].set_title(companyData[\"StockName\"])\n",
    "\n",
    "\ttweetJumpDates = companyData[\"TweetJumpDates\"]\n",
    "\tstockJumpDates = companyData[\"StockJumpDates\"]\n",
    "\tdaysToJumpList = GetDaysBetweenTweetAndStock(tweetJumpDates, stockJumpDates)\n",
    "\n",
    "\tavgGapBetweenTweetJumps = AvgDaysBetweenJumps(tweetJumpDates)\n",
    "\tavgGapBetweenStockJumps = AvgDaysBetweenJumps(stockJumpDates)\n",
    "\t\n",
    "\taxs[i].hist(daysToJumpList, 50,  label=\"Time between tweet jump to stock jump \")\n",
    "\n",
    "\taxs[i].bar([avgGapBetweenTweetJumps], [4], color=['red'], label=\"avg Gap Between Tweet Jumps\")\n",
    "\taxs[i].bar([avgGapBetweenStockJumps], [4], color=['green'], label=\"avg Gap Between Stock Jumps\")\n",
    "\n",
    "\tavgGap = sum(daysToJumpList) / len(daysToJumpList)\n",
    "\taxs[i].bar([avgGap], [4], color=['magenta'], label=\"avg Gap Between Stock Jumps and tweets\")\n",
    "\n",
    "\n",
    "\taxs[i].set(xlabel='Number of days between jumps', ylabel='Count')\n",
    "\taxs[i].label_outer()\n",
    "\taxs[i].xaxis.set_ticks(np.arange(0, 300, 7))\n",
    "\taxs[i].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "### Used for validation Of Data\n",
    " - https://www.google.com/finance/?hl=en\n",
    " - https://twitter.com/\n",
    "\n",
    "### Useful Docs\n",
    " - https://github.com/JustAnotherArchivist/snscrape\n",
    " - https://pypi.org/project/yfinance/\n",
    " - https://pandas.pydata.org/docs/reference/frame.html\n",
    " - https://matplotlib.org/stable/gallery/index.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e03891b53087abc58e95159b2c70e6ab6b812f7c74a522802c5c9765390d0ed"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
